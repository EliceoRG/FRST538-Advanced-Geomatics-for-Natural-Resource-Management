
```{r echo=FALSE}
yml_content <- yaml::read_yaml("chapterauthors.yml")
author <- yml_content[["lidar-for-forest-management"]][["author"]]
```

# Lidar for Forest Management {#lidar-for-forest-management}

Written by
```{r results='asis', echo=FALSE}
cat(author)
```

## Lab Overview {.unnumbered}

The aim of this lab is to use LiDAR data from the Malcolm Knapp Research Forest (MKRF) explore a discrete return LiDAR dataset and become familiar with basic functions in the R package lidR and .las data sets. You will create digital terrain models (DTMs), digital surface models (DSM), canopy height models (CHMs) and visualize the point clouds in R.  Additionally, we will explore individual tree detection and examine how LiDAR can be used to model forest attributes.

------------------------------------------------------------------------

## Learning Objectives {.unnumbered}

-   Create a DEM from LiDAR point data in both 2D and 3D
-   Calculate meaningful metrics from points clouds
-   Apply an OLS model to predict forest forest attributes with LiDAR metrics 
-   Practice variable selection in modeling 
-   Apply individual tree detection using point and CHM to create a map of tall trees from a LiDAR point cloud

------------------------------------------------------------------------

## Deliverables {#lab3-deliverables .unnumbered}

<input type="checkbox" unchecked> Turn in the answers to the questions given in the lab on Canvas.</input>

------------------------------------------------------------------------

## Data {.unnumbered}

We will be working with LiDAR data collected over the Malcolm Knapp Research Forest (MKRF) in Maple Ridge, BC.  There are three files associated with this lab:

- MKRF_Data_Specifications.pdf: A document describing the LiDAR data collection
- LAS folder: The LiDAR data files in  .LAS  format. .LAS is the standard file type for LiDAR data
- MKRF_Aerial_Photo.tif: An orthophoto of our study area, which was collected at the same time as the LiDAR data
- MKRF_lidar: a single lidar file of the MKRF
- Empty folders: you will save your outputs to these folders 
- Variety of CSVs used to create models 
- Lab6_Script.r: A script to process .las files using R and the lidR library. 

------------------------------------------------------------------------

## Software {.unnumbered}

- R/RStudio

For this lab you will need to download R and RStudio. Rstudio is an open source integrated development environment (IDE, think fancy notepad) for the R programing language. It includes a console, syntax editor for code execution and will generally improve your R programing experience. If you need to install [R](https://cran.rstudio.com/) and [R studio](https://posit.co/download/rstudio-desktop/) click the respective links or ask your TA. If you already have them installed on your computer continue to the next section of the lab. 

**Warning:** This lab (and LiDAR processing in general) is computationally demanding. I have reduced the study area and used less rigorous algorithms to speed up processing time. Generally speaking a single line of code should not take longer than 5 minutes to run.  

------------------------------------------------------------------------

## Lab Set up {.unnumbered}

For this lab we will use two R packages: lidR and tidyverse

LidR contains the functions used to process the lidar data, while tidyverse is a generic set of function to help with data wrangling and manipulation. 

To run lines of code within Rstudio you can use **ctrl+enter**. This will run either the highlighted text or the line of code your cursor is on. 

To begin open RStudio and check if the packages are installed 

```{r, echo=TRUE, eval=FALSE}
#Install and/or load packages 

#install.packages("lidR")# Run if not installed 
#install.packages("tidyverse")# Run if not installed

library(lidR)
library(tidyverse)

```

Set your working directory to the folder that contains your Lab 3 data using the setwd() function. Doing so will save you time during file uploading/exporting throughout the lab. If you forget your working directory after it is set, you can type getwd() into the console and R will print it for you.

```{r, echo=TRUE, eval=FALSE}
# This is the only section of the lab you will need to fill out yourself. 
# It should look something like this setwd("C:/Users/Spencer/FRST538/Lab6/Data") but using the path for your computer 
setwd("C:/{file_path}")
```

------------------------------------------------------------------------

## Task 1: Load RGB Imagery {.unnumbered}

When working with any airborne spatial data, it is useful to examine an RGB photo of the study site (MKRF_Aerial_Photo.tif). Take time to locate different land cover features (roads, lakes, streams, etc.) and to understand how vegetation structures are distributed. Is there wall-to-wall forest at the site? Are there any cut-blocks or major roads that could act as reference points during processing? Will the location of water bodies compromise random plot selection?

For this lab, these considerations have already been made. At the very least, you can use the RGB image of Malcolm Knapp Research Forest (MKRF) to gain perspective of our study area. We will use the brick function from the package raster, which was loaded automatically when we loaded lidR. This function creates a RasterBrick object from a multi-layer (or band) file (Figure \@ref(fig:03-rgb-malcolm-knapp)).

You can explore the details of any function that we use in this lab by typing a question mark `?` followed by the function name into the console: `?brick`. The information will appear in the Help pane of your RStudio interface.

Once the RasterBrick has been read into R, we will explore the objects structure using the `str()` function and plot the RGB image using the plot function. 

```{r, echo=TRUE, eval=FALSE}
#Load RGB image and explore structure
rgb_mkrf <- brick('Aerial_Photo/MKRF_Aerial_Photo.tif')
str(rgb_mkrf) 
plotRGB(rgb_mkrf)
```

```{r 03-rgb-malcolm-knapp, out.width= "100%", echo = FALSE, fig.align='center', fig.cap=fig_cap}
    knitr::include_graphics("images/03-rgb-malcolm-knapp.png")
    fig_cap <- paste0("RGB image of the Malcolm Knapp Research Forest")
```

------------------------------------------------------------------------

## Task 2: Load and Explore the LiDAR Data {.unnumbered}

First, we need to load the file MKRF_lidar.las from your /Data folder. This file contains the lidar data you will need to complete this section of the lab. We will use the function `readLAS` to load the .las file. Once the file is loaded it should appear in your Environment as a ‘Large LAS’. It is best practice to use the function `lascheck` to conduct a deep inspection of a LAS object. This function will print a report for you to examine.

You can explore the structure of objects in your Environment using the `str()` base function in the Console. You can also explore the contents of a single column within the object by typing the objects name followed by a dollar sign and the column name: `object$column`.

_HINT: If you wish to learn the details of any function associated with your loaded packages, you can type a question mark followed by the functions name: `?function_name`_

```{r, echo=TRUE, eval=FALSE}
#Load lidar data
las_mkrf <- readLAS("./MKRF_lidar.las") #read the specified .las file  
las_check(las_mkrf) #check the LAS object
str(las_mkrf) #compactly display the internal structure of las_mkrf
print(las_mkrf)
```

##### Q1: Examine the lascheck output. Is the data normalized? Has a ground classification been completed already? {.unnumbered}

##### Q2: Examine the output of str(). How many first returns are there? What is the maximum z value? {.unnumbered}

##### Q3: Examine the output of print(). What is the density of this data? Is that high, low, or somewhere in the middle? {.unnumbered}

Upon exploring the lascheck output you will notice that there are some duplicated points. This is not a problem for our analyses, but it is worth noting. For example, the statement: There were 6 degenerated ground points. Some X Y coordinates were repeated but with different Z coordinates means that multiple pulses measured the same location and returned different Z, or elevation, coordinates. This could be caused by overlapping flight paths or pre-processing errors, but lidR has the capacity to deal with these issue. Overall, our lidar data looks good. Before moving on let us remove the duplicated points.

```{r, echo=TRUE, eval=FALSE}
#remove duplicated Z (eleavation) points
las_mkrf <- filter_duplicates(las_mkrf) #remove the duplicated points
#confirm duplicates removed
las_check(las_mkrf) #run lascheck a second time to ensure duplicates removed
```

Now that we have a general understanding of our lidar data, it is time to start visualizing! We will explore 2D and 3D plots of our data to gain an even deeper understanding of our study site before undergoing some basic manipulation. 

First, we will visualize the density of lidar points across our site. You will notice that some areas have a higher density than others. This is likely due to the flight path of the aircraft that collected our data and probably causes some of the duplicated points that were highlighted by `lascheck`. Next, we will plot las_mkrf in 3D using the plot function. In lidR, this function can produce plots 

in 2D or 3D, depending on structure of the input object. LASobjects are plotted in 3D, while Raster objects are 2D.

```{r, echo=TRUE, eval=FALSE}
#Visualize point density 
density_mkrf <- grid_density(las_mkrf, res = 2) #create a map of the point density
plot(density_mkrf) 

#Visualize las_mkrf in 3D 
plot(las_mkrf)
```

Wonderful! We can now explore lidar data in 3D by clicking and scrolling through the RGL plot. Due to the size of the file this exploration may be relatively slow (Figure \@ref(fig:03-rgl-plot)). 

```{r, 03-rgl-plot, out.width= "100%", echo = FALSE, fig.align='center'}
    knitr::include_graphics("images/03-rgl-plot.png")
```

------------------------------------------------------------------------

## Task 4: Read Multiple LAS Files into LAScatolog object  {.unnumbered}

Once you have a basic understanding of the study area, it is time to load your lidar data. The file type used in this course is LAS, which is an industry standard format for storing lidar data. In the future, you may also work with other file formats, such as LAZ. You can explore the differences between these formats [online](https://rapidlasso.de/five-myths-about-las-laz-and-optimized-las/).

In the last task, we loaded a single LAS object  and plotted it. For the remainder of this lab, we will load 20 tiles collected over MKRF into a LAScatalog object. This will enable us to manage and process our entire data set as one object, rather than 20 separate files. We can use lascheck to perform a deep inspection of the LASCatalog object or use `summary()` for a more succinct report.

We can also plot a LAScatalog object to observe how our tiles are arranged. Since the LASCAtalog object is a SpatialPolygonsDataFrame, it is possible to use the `spplot` function to visualize certain characteristics of our data. For example, the line `spplot(cat_mkrf, "Min.Z")` will plot catalog and color each tile based on it’s lowest elevation (Z) value (Figure \@ref(fig:03-rgl-plot)).

```{r, echo=TRUE, eval=FALSE}
#Create LAScatalog object from MKRF las tiles
cat_mkrf <- readLAScatalog("LAS")
las_check(cat_mkrf)
summary(cat_mkrf)
plot(cat_mkrf)
```

Notice that there are overlapping tiles. This indicates that there may be duplicate points within our data. We can confirm this by applying the `lascheck` function to a single tile.

```{r, echo=TRUE, eval=FALSE}
#Examine single .las tile to determine duplication
tile_1_mkrf <- readLAS("LAS/MKRF_Tile1.las")
lascheck(tile_1_mkrf)
```

It appears that there are 421 duplicate points. These may impact both the accuracy of our analysis and the processing speed. Let us remove them using the `filter_duplicates` function and store the filtered tiles in a new folder named ‘Filtered’ (this folder has already been created for you). Once complete, confirm that the duplicates were removed.

```{r, echo=TRUE, eval=FALSE}
#Set the output directory for the filtered .las data
work_dir = getwd() #create a filepath to your working directory
opt_output_files(cat_mkrf) <- paste(work_dir, "/Filtered/filtered_mkrf_{ID}", sep = "")#location to store duplicate points
cat_mkrf <- filter_duplicates(cat_mkrf) #remove duplicate points and speed up processing

#confirm that duplicates were removed for Tile 1
filtered_t1_mkrf <- readLAS("Filtered/filtered_mkrf_1.las") #load from stored file
lascheck(filtered_t1_mkrf)
```

Success! We can now proceed with our analysis. Let us read all filtered LAS data into a catalog and perform the usual checks.

```{r, echo=TRUE, eval=FALSE}
#read filtered .las into LAScatalog
filtered_cat_mkrf <- readLAScatalog("Filtered")
summary(filtered_cat_mkrf)
plot(filtered_cat_mkrf)
spplot(filtered_cat_mkrf, "Min.Z")
```

------------------------------------------------------------------------

## Task 4: Create Digital Elevation Model (DEM) {.unnumbered}

Now that we have practiced some of the basic visualization, reading, and cleaning techniques on raw lidar data it is time to move onto processing. The first thing we will do is create a digital elevation model (DEM). A DEM represents the Earth's surface without vegetation, which we will refer to as "ground". Although this seems like an intuitive statement, there is the potential for logs, rocks and other non-soil objects to be included as ground points. These won't cause issues for our analyses, but it is important to use precise language when working with common words like "ground". Never assume that your definition of something is a global truth. It is also important to consider that a DEM can also be referred to as a digital terrain model (DTM). Take time to understand that both terms refer to a surface model of only ground points. Instructors/colleagues could use DEM and DTM interchangeably.

We will create and visualize a DEM in both 2D and 3D. The 3D version will be created by filtering out all points that are not classified as ground (Classification: 2). For the 2D DEM, we will use the `grid_terrain` function on our LidR catalogue. This function enables the user to select the output resolution and the algorithm that computes spatial interpolation. It is recommended that you take time to understand the three algorithms in highlighted in the `grid_terrain` 'Help' file.

```{r, echo=TRUE, eval=FALSE}
#DEM
#3D
dem_3D_mkrf <- filter_poi(las_mkrf, Classification == 2)
plot(dem_3D_mkrf) #zoom in to notice no trees
plot(las_mkrf) # compare to dem_3dmkrf
#demCatalog
dem_allLAS_mkrf <- grid_terrain(filtered_cat_mkrf, 2, tin()) #why use this method? check warnings: why might points get degenerated DON'T PANIC AT 'WARNING'
# create Colour ramp for visualization 
col_1 <- height.colors(50)
plot(dem_allLAS_mkrf, col = col_1)
```

##### Screenshot 1: Submit the output of this plot() as a screenshot. {.unnumbered}

With the DEM complete we can normalize our LAScatalog. Each tile will be normalized separately, which means there will be one normalized output tile for each input. lidR requires an output directory to be specified. A folder named ‘Normalized’ has already been created for you. Once the output file directory is set we will use `lasnormalize` to conduct the normalization of all our MKRF lidar tiles.

```{r, echo=TRUE, eval=FALSE}
#Normalize catalog with DEM
norm_tiles_mkrf <- lasnormalize(filtered_cat_mkrf, dem_allLAS_mkrf)

#define LAScatalog engine options
opt_output_files(filtered_cat_mkrf) <- paste(work_dir, "/Normalized/norm_mkrf_{ID}", sep = "")

#normalize all tiles in cat_mkrf with the DEM 
norm_tiles_mkrf <- normalize_height(filtered_cat_mkrf, dem_allLAS_mkrf) #check your folder when complete 

#check to see if the normalization worked
norm_mkrf_1 <- readLAS("Normalized/norm_mkrf_1.las")
plot(norm_mkrf_1) 

```

Explore the 3D rgl plot that is produced by the last line in the above code block (Figure \@ref(fig:03-normalized-tile-mkrf)). Does anything look abnormal to you?

```{r 03-normalized-tile-mkrf, out.width= "100%", echo = FALSE, fig.align='center', fig.cap=fig_cap}
    knitr::include_graphics("images/03-normalized-tile-mkrf.png")
    fig_cap <- paste0("Normalized tile 1 from MKRF.")
```

##### Q4: There appears to be an outlying data point located high above the others. Explain why this would cause issues during our processing and an example of what would cause this outlier. {.unnumbered}

------------------------------------------------------------------------

## Task 5: Create a Canopy Height Model {.unnumbered}

We can explore the effect of this outlier by generating a canopy height model (CHM) with the grid_canopy function. Mathematically, a CHM is simply the DEM (or DTM, remember these terms are interchangeable!) subtracted from the DSM. This subtraction can be called 'normalization', differing from the normalization conducted using `lasnormalize` in the sense that the output is 2D. The CHM is an important surface model as it allows us to analyze objects above a common ground value (0m). This 2D output should have a maximum height around that of the highest tree on site (~ 60m).

The `grid_canopy` function is used to generate CHMs, which has three user defined arguments. The first, `las` refers to the input data (LAS or LAScatalog). The second, `res` refers to the spatial resolution. The third argument, `algorithm` specifies the algorithm that will be used to compute the digital surface model. There are three algorithms compatible with grid_canopy:

   `p2r` is a point-to-raster method. It returns the maximum height value of the point cloud for each area, which is defined by the spatial resolution. This is a relatively fast algorithm, but can generate 'pits', or areas of ‘NoData’ in the CHM where there are no returns present. You can add a subcircle argument within the p2r algorithm to try and remove pits. This method replaces each lidar point with a disk of a specified size and increases return coverage. If you use this method, be aware that issues related to computation speed may arise.
   
   `dsmtin` is a triangulation-based method that creates a triangular irregular network (TIN) by interpolating only the first returns before generating a 2D grid. This method eliminates empty pixels, but can still generate pits within crowns as some first returns inevitably reach the ground.
   
  `pitfree` is a triangulation-based method that conducts multiple triangulations at various heights. Developed by Khosravipour et al. (2014), the pitfree algorithm ensure there are no empty cells or deep pits. Shallow pits may remain, but can be removed by adding a subcircle argument. Although the most sophisticated, this algorithm is computationally intense.

Since we know that our lidar data has 3.1 returns · m^-2^, we can assume that a spatial resolution of 2 m will ensure that every cell in our raster contains at least one return. This, along with the advantage of quick processing (~1 min for entire site), allows us to use the stock `p2r` algorithm without worrying about pits. Feel free to use `dsmtin()` (~5 min) or `pitfree()` (~10 min) for the following steps, just ensure you annotate your code accordingly.

```{r, echo=TRUE, eval=FALSE}
#Create CHM for normalized Tile 1
chm_mkrf_1 <- grid_canopy(norm_mkrf_1, 2, p2r()) #las, resolution, algorithm
plot(chm_mkrf_1, col = col_1) #plot in 2D
plot_dtm3d(chm_mkrf_1) #plot in 3D
```

It is clear that the normalization was not perfect as the maximum height of the CHM is well above our expected maximum height. This issue is caused by the outlier visible in the 3D plot. It also looks like there are some points below 0m. To remedy these issues, we can use the LAScatalog options engine to filter out data points below 0 m and above 65 m.

```{r, echo=TRUE, eval=FALSE}
LAScatalog options engine to filter out data points below 0m and above 65m.
#read normalized las into catalog to continue processing
norm_cat_mkrf <- readLAScatalog("Normalized")

#add LAScatalog enginge option to filter undersired data points
opt_filter(norm_cat_mkrf) <- '-drop_z_below 0 -drop_z_above 65'

#ensure the entire study area was processed
plot(norm_cat_mkrf)
summary(norm_cat_mkrf)
```

It seems like all of the normalized LAS files were loaded correctly, but we still need to confirm that the outliers were removed by the filter we applied. To do this we will create another CHM. Instead of making the CHM for Tile 1, we will create a CHM for the entire site.

```{r, echo=TRUE, eval=FALSE}
#Create CHM for all normalized MKRF Tiles
chm_mkrf <- grid_canopy(norm_cat_mkrf, 2, p2r()) 
plot(chm_mkrf, col = col_1) #plot in 2D
#plot_dtm3d(chm_mkrf) #plot in 3D *DON'T RUN IF COMPUTER IS SLOW*
```

Excellent! The heights in our CHM are as expected. If you are interested in exploring how the CHM relates to the RGB image of the area, please compare the two now.

##### Screenshot 2: Include an image of your CHM. {.unnumbered}

##### Q5. What do you think the large empty locations in the CHM represent? {.unnumbered}

------------------------------------------------------------------------

## Task 6: Extract Metrics From the Point Clouds  {.unnumbered}

Now that we have confirmed that our filter works at the catalog level, it is time to extract some plots. First, let us extract a single plot. We use the lasclip function to do this. Specifically, we will use lasclipCircle to extract circular plots from our lidar data. Check out the details of this function in the lidR documentation provided at the start of the lab. We will extract a plot with the following characteristics:

**X = 530118.9**

**Y = 5462882.0**

**Radius = 25**

```{r, echo=TRUE, eval=FALSE}
#Extract single plot
single_plot_mkrf <- clip_circle(norm_cat_mkrf, 530118.9, 5462882.0, 25)
plot(single_plot_mkrf) 

```

Now we will extract multiple plots using a for loop. The location of each plots is provided in the ‘Plot_Table.csv’ within the ‘/Plots’ folder of your lab data. Plot radii should be 30 m. The for loop is annotated for you so you can understand what each line of code is doing.

```{r, echo=TRUE, eval=FALSE}
plot_table <- read_csv("Plots/Plot_Table.csv")
radius <- 30 #the radius for lasclipCircle()
for(i in 1:nrow(plot_table)){ #run the loop until i = the number of rows in 'plot_table' (20)
  plot_cent <- c(plot_table$X[i], plot_table$Y[i]) #extract plot center
  plot_las <- clip_circle(norm_cat_mkrf, plot_cent[1], plot_cent[2], radius) #clip plot from norm_cat_las
  output_file <- paste("Plots/MKRF_Plot_", i, ".las", sep = "") #output directory as string
  writeLAS(assign(paste("MKRF_Plot_", i, sep = ""), plot_las), output_file) #write'MKRF_Plot_i' to output dir.
}
```

At this point, you can explore individual plots in 3D, which should be loaded in your Environment, using `plot()`.

Now that we have extracted our plots it is time to compute some standard cloud metrics. To do this, we will need to analyze each plot individually. We also need to make sure that we are only including the first returns. This will ensure our metrics do not include points that have been scattered throughout the canopy. Let’s also remove all points below 2 m, as objects below this height could be non-tree (shrubs, fallen logs, etc.). We do not need to worry about outliers as we removed them during the normalization process. Explore the Console output of the line plot_1_metrics for a brief overview of the metrics being created.

```{r, echo=TRUE, eval=FALSE}
#Calculate cloud metrics for all plots
#there are special considerations: only first returns between 2m and 65m
#create empty dataframe
mkrf_plot_metrics <- data.frame() 

#For loop to calculate cloud metrics for all plots and add them to 'mkrf_cloud_metrics'
for(i in 1:nrow(plot_table)){ #for loop == number of rows in plot_table (20)
  plot <- readLAS(paste("Plots/MKRF_Plot_", i, ".las", sep= ""), filter = "-keep_first -drop_z_below 2")
  metrics <- cloud_metrics(plot, .stdmetrics) #compute standard metrics
  mkrf_plot_metrics <- rbind(mkrf_plot_metrics, metrics) #add the new 'metrics' to 'mkrf_cloud_metrics'
}
```

Take a few minutes to scroll through mkrf_cloud_metrics. You can export this dataframe to a .csv if you wish. There are many metrics calculate by this function and you can explore them in detail by typing `?cloud_metrics` into your console and navigating through the interactive Help page. Below are some basic details relating to cloud_metric notation that you can find by clicking on `stdmetrics` in the aforementioned Help page

```{r, echo=TRUE, eval=FALSE}
Details

The function names, their parameters and the output names of the metrics rely on a nomenclature chosen for brevity:

z: refers to the elevation

i: refers to the intensity

rn: refers to the return number

q: refers to quantile

a: refers to the ScanAngleRank or ScanAngle

n: refers to a number (a count)

p: refers to a percentage
```

Let us take a closer look at the metrics for plots 1 and 16. Use the code below to create a new table called ‘compare_metrics’ to make your exploration easier:

```{r, echo=TRUE, eval=FALSE}
#Compare Plots 1 and 16
compare_metrics <-  mkrf_plot_metrics[c(1,16),]#create df of plots1,16)
head(compare_metrics)

#Export mkrf_plot_metrics as .csv in Plots folder
write_csv(mkrf_plot_metrics, "Plots/mkrf_plot_metrics.csv")
```

##### Q6: Why is the 50th heigh percentile so different between the two plots while the 95th height percentile is so similar? {.unnumbered}

------------------------------------------------------------------------

## Task 7: Stem Volume Models with lidR {.unnumbered}

In order to estimate forest attributes across our study area, we must first develop relationships between our lidar metrics and field measured forest attributes. For our 20 plots, we have estimates of net stem volume m^3^ · ha^-1^ that were measured in the field in 1996. Plot locations were evenly spaced across the study area, and volume was estimated at each plot using a variable-radius approach (basically, the further a tree is from the plot center, the larger it must be to be included in the plot).

These plot data violate many of our lidar best practices for estimating forest attributes. First, there is a long time lag between our field measurements (1996) and our lidar data collection (2010). Ideally, there would be a maximum of one or two years difference. Changes in forest conditions (growth and mortality) could lead to large differences in net stem volume between 1996 and 2010 at these plot locations. Second, because forest conditions were not used to guide the selection of plot locations, they do not capture the full range of structural variability across our study area. Therefore, our models may only be applicable to a small range in forest conditions. Third, because the plots are variable radius, there is not a fixed plot size that we can extract from our lidar data to associate with each plot.

All of these issues will influence our ability to develop strong relationships between lidar metrics and forest attributes, which significantly limits the value of this lidar dataset for forest attribute estimation. These issues are sometimes the reality when we work with lidar data, as resources may not be available to collect the proper field data within several years of the lidar flight. Therefore, even with a high quality lidar dataset, the accuracy of our estimates will largely depend on the quality of our plot data. Keep these issues in mind when we begin to develop models.

There are many statistical approaches to predicting forest attributes with lidar metrics. Here, we will use ordinary least-squares regression. As you work through this section, remember that this is only one approach of many, and there is no single correct approach. In your own research, you should spend time considering the various modelling approaches, and the advantages and disadvantages of each.

Within ordinary least-squares regression, there are more decisions we must make. Which lidar variables do we want to include in our model? Do we want to transform the variables before developing the model (e.g., log transform)?

We first select an initial subset of lidar variables that we would like to include in model development. Many lidar metrics are highly correlated with one another, and would therefore provide little unique information for predicting forest attributes. Additionally, with so many variables, some of these metrics are likely to explain noise in our model. This may lead us to conclude that our model performed well, when in fact we simply overfit our data. Therefore, we typically select a small subset of metrics that we feel capture the most important information in the lidar point cloud. Most lidar metrics fall into one of three groups: 1) those that describe vegetation cover, 2) those that describe vegetation height, and 3) those that describe vegetation complexity (e.g., variation in return height). When selecting your subset of metrics, it is important to include at least one metric from each of these classes.

Once our initial subset of variables is selected, we can begin to develop our model. This may involve transforming our data to improve the relationship between lidar metrics and forest attributes. Various transforms can be tested, including taking the natural log of the X variables (or both X and Y), or squaring the X variables. You may find that no transformation is necessary to establish a strong model.

Additionally, there are various approaches to determine which of our initial variables should actually be included in our final model. In this section, we will use a simple forward variable selection process to accomplish this. We begin by developing a model with each lidar metric individually. We select the most significant metric (the one with the lowest p-value) as the first variable in our model. Next, we develop a set of models that includes our first variable in combination with each other metric. We select the most significant of these metrics as our second variable in our model. We continue adding variables in this way until no addition to the model is significant (p < 0.05, or a p value of your choosing). Forward selection is a straight forward and simple approach to selecting variables. Again, in your own research, you should explore the advantages and disadvantages of various variable selection approaches before building models.

Once our model is developed, we can apply the model ‘wall-to-wall’ across our study area. This is accomplished by calculating the lidar metrics on a grid across our entire study area, and applying the relationship we developed.

#### Part A {.unnumbered}

We will begin by exploring our lidar metrics and selecting an initial subset of metrics to include in model development. In lab 2, we calculated lidar metrics for each plot using a radius of 30 m. We will use these metrics to develop our model.

```{r, echo=TRUE, eval=FALSE}
#Read in relevant .csvs
plot_table <- read_csv("Plots/Plot_Table.csv")
mkrf_plot_metrics <- read_csv("Plots/MKRF_Plot_Metrics.csv")

#Add column to "mkrf_plot_metrics' called Plot_ID (join key)
mkrf_plot_metrics$Plot_ID = 1:20

#Join 'Plot_Table' and 'MKRF_Plot_Metrics' into 'data_table'
data_table <- plot_table %>% 
  full_join(mkrf_plot_metrics)

```

This script is divided into 4 Parts (A - D). Part A contains the required inputs for the script to run correctly. These inputs are filled in, but make any adjustments if your filenames are different. Part B reads in our Plot_Table.csv and MKRF_Plot_Metrics.csv files, and organizes them. The Plot_Table.csv file contains the net stem volume estimates for each plot, while the MKRF_Plot_Metrics.csv file contains our calculated lidar metrics. A number of metrics are not of any interest to us (e.g, intensity metrics), so if you wish an extra challenge you can remove them from dataTable. At the end of Part A, our Plot_Table.csv and MKRF_Plot_Metrics.csv are combined into a single master table called dataTable. Run Part A of the script by highlighting them, and hitting Ctrl + Enter. You should see the commands executed in the Console window in the lower left of the R studio window.

Once Part A is executed, scroll down to Part B. This section of the script is designed for us to explore our lidar metrics and select a subset of metrics for model development. Begin by highlighting the line colnames (dataTable) and pressing Ctrl + Enter to run it. This will generate a list of the column names in dataTable:

#### Part B {.unnumbered}

```{r, echo=TRUE, eval=FALSE}
colnames(data_table)
plot(Net_Volume ~ zq50, data = data_table)
```

This table contains the forest attribute we want to predict (Net_Volume) in addition to all of our lidar metrics. Revisit the `cloud_metrics` Help page to review each metric.

We will start by exploring the relationship between our lidar metrics and volume. Run the following line to plot the relationship between the 50th height percentile and net stem volume:

```{r, echo=TRUE, eval=FALSE}
plot(Net_Volume ~ zq50, data = data_table)
```

data = data_table tells R to look in dataTable for columns named Net_Volume and zq50.

Next, we will fit linear models between our lidar metrics and volume. Run the following lines to create the model and display a summary of the model you just fit. Find the value in the summary to assess how much variability in volume the 50th percentile of height percentile could explain.

```{r, echo=TRUE, eval=FALSE}
model = lm(Net_Volume ~ zq50, data = data_table)
summary(model)
```

We can have two or more predictors in our linear model by adding a + sign between the predictor names. Run the following lines:

```{r, echo=TRUE, eval=FALSE}
model = lm(Net_Volume ~ zq50 + zq90, data = data_table)
summary(model)
```

Notice how the R^2^ value barely improved. This is because the 50th and the 90th height percentiles are highly correlated with one another, so adding the 90th height percentile does not add much additional information. Run the line model$coefficients. This will display the coefficients of our model. Using these coefficients, we can write our linear model as follows:

${Volume = 48.31 * zq50 - 22.91 * zq90 - 95.39}$

Notice how there is a negative relationship between volume and the 90th percentile in our model. This means that as the 90th percentile increases, our estimates of volume will decrease (not the relationship we would like to see!). This is a sign that our predictors are highly correlated, and therefore the 90th percentile is just fitting noise. Always check that your models make sense.

Remember that height, cover, and complexity metrics each provide unique information about the vegetation in the plots. While the relationship might not be strong between a certain metric and volume, that variable might provide unique information to the final model that is not provided by another, stronger variable. Therefore, do not throw out a variable simply because it has a weak relationship on its own. If it turns out that the variable does not explain a significant amount of variation, we will throw it out in the model development stage.

We can also look at the correlation between our lidar metrics. Run the line:

```{r, echo=TRUE, eval=FALSE}
pairs(~ zq10 + zq25 + zq50 + zq75 + zq90, data = data_table)
```

This will generate a matrix of scatterplots between the five height percentiles that are listed (zq10, zq25, zq50, zq75, zq90). Notice how all of these height percentiles are highly correlated with one another. We would not want more than one or two of these in our model, as they basically provide the same information.

Run the line:

```{r, echo=TRUE, eval=FALSE}
pairs(~ zq90 + pzabove2 + zentropy + zskew, data = data_table)
```

This will display more scatterplots, but between variables that are less correlated with each other. The goal is to find a set of metrics where each metric provides unique information (i.e., not strongly correlated to any other variable). There may be some correlation between variables, you just don’t want too much of it. Notice how we have at least one cover metric, one height metric, and one complexity metric (i.e., variability metric).

##### Screenshot 3: Export the result of `pairs()` as an image and insert into your submission. {.unnumbered}

##### Developing a statistial model {.unnumbered}

After selecting a handful of initial metrics, we can develop our model to predict net stem volume. We will try two example sets of initial metrics. As noted in the introduction, we will use a forward selection process to develop our final model.

Go to Example 1 of Part C in the R script. For this example, our initial variables are zq90, pzabove2, zentropy, and zskew. Run through this example line by line in the R script to build our first model using forward selection. Read the comments in the script to understand how this process works.

Once you complete Example 1, move on to Example 2. The only change here is that zq90 has been replaced by zq25. Run through example 2 line by line.

You will notice that the small change from Example 1 to Example 2 (Using zq25 instead of zq90) completely changed our final model. Here are the final two developed models below. Look at the model coefficients in R to confirm that these models are correct.

**Model 1:**

${Volume = 17.44 * zq90 - 1616.33 * pabove2 - 161799.33}$

**Model 2:**

${Volume = 24.96 * zq25 - 139.45}$

The models explain the same amount of variance in volume (similar R2 values). However, model 2 is the stronger candidate as it uses only one predictor (the less complicated the model, the better).

Now that we have developed a model, we will apply the model over our entire study area. To do this, we must calculate lidar metrics over the entire study area on a grid. If we use model 2, we only have to calculate the 25th percentile.

We will use the `grid_metrics` function in lidR and the `calc` function in raster to do this. Explore the details of `grid_metrics` and `calc` before beginning this step. Notice that `grid_metrics` calls for an internal function to be applied to each pixel in the raster. It seems intuitive to apply a function created directly from the coefficients of Model 2, but we first need to calculate zq25 for each pixel. To do this we will calculate the standard metrics for the entire study site using `.stdmetrics_z` as our function. For the sake of time, the spatial resolution is set to 10 m.

Once the processing is complete, we should plot `grid_metrics`.

```{r, echo=TRUE, eval=FALSE}
grid_metrics_mkrf <- grid_metrics(norm_cat_mkrf, .stdmetrics_z, 10)
plot(grid_metrics_mkrf)
```

Wow! Look at all the rasters we have created. We can plot individual rasters using `plot(grid_metrics_mkrf, "zentropy")`. For this lab, we want to extract the zq25 raster from this RasterBrick. We can do that by using the subset function in the raster package. Let us plot the extracted RasterLayer to confirm it was successful.

```{r, echo=TRUE, eval=FALSE}
zq25_mkrf <- subset(grid_metrics_mkrf, "zq_25")
plot(zq25_mkrf)
```

Excellent. Now that we have the value of the 25th percentile for each pixel across the study area we can apply Model 2. To do this, we first need to write a function that will apply this equation to all pixels in zq25_mkrf. The function we will create is based on the coefficents derived from Model 2:

```{r, echo=TRUE, eval=FALSE}
f <- function(x){
24.96*x -139.45
}
```

Now, use `calc` to apply the model to the entire study area. Add this Figure into your PDF as Screenshot 4.

Done! You have successfully developed and applied a model to predict net stem volume at Malcolm Knapp Research Forest. Hooray!

##### Q7: Does a reasonable high ${R^2}$ value necessarily mean this is a good model? Do you think our estimates of volume are accurate? How could we test the accuracy of our model? {.unnumbered}

##### Q8: Cover is typically an important metric for predicting forest attributes such as volume (the higher the cover, the higher the volume). Why do you think cover was not an important variable in our model? Consider the location of the plots in your answer. Use the following line to view a scatterplot between cover and volume in R. {.unnumbered}

```{r, echo=TRUE, eval=FALSE}
plot(Net_Volume ~ pabove2, data = data_table)
```
------------------------------------------------------------------------

## Task 9 {.unnumbered}

#### Part A) Detect individual trees from a point cloud {.unnumbered}

For the first part of this task, we will detect individual trees using only the lidar point cloud data. To start, we will focus on Plot 1. This plot is located in a clearcut, where a number of trees were left standing. The crowns of these trees are clearly separated, so this plot is a good place to test our individual tree detection (i.e., we can compare detected trees against those we can see in the plot).

To detect individual trees, we will use the segment_trees function. There are multiple algorithms available in segment_trees and we recommend you explore each. For this section of the lab, we will use the li2012 algorithm. Please review the Li et al., 2012 paper if you wish understand how this algorithm works. 

To run, this function requires multiple arguments. The first arguments are the adaptive thresholds, or dt1 and dt2 . These thresholds determine the distance that one point can be from another to be included in the same tree. They are considered adaptive because the algorithm chooses to use dt1 or dt2 based on the height of the seed point, which is a technical term for tree top. To be accurate, it is necessary to determine the spacing of trees within the area being classified. This can cause challenges because it is difficult to calculate tree spacing without first accurately segmenting individual trees.

The argument R refers to the search radius for points around the local maxima, which has been determined by Li et al., 2012 to be 2m in most cases. 
The argument Zu determines which threshold is used by evaluating the height of the seed point. If Zu = 15 , then any seed point higher than 15m will use dt2 . Any seed point lower than 15m will use dt1. Li et al., 2012 have determined default values through trial and error: dt1 = 1.5 , dt2 = 2 , Zu = 15 , and R = 2

For this lab, we will use the default values determined by Li et al., 2012 for all arguments: dt1 = 1.5 , dt2 = 2 , R = 2 , Zu = 15 and speed_up = 10 . We will also use the default value for hmin , which is 2m, as it follows with previous labs in excluding points below 2m from our analyses.

Run **Part A** of the script now.

```{r, echo=TRUE, eval=FALSE}
#Part A) Point Cloud based 
norm_mkrf <- readLAScatalog("Normalized")
opt_filter(norm_mkrf) <- "-keep_z_above 2 -drop_z_above 65"
plot(norm_mkrf)

#Plot Extraction
#load csv
tree_meas <- read_csv("Tree_Measurements.csv")
plots <- read_csv("Lab6_Plots.csv")

#extract plots
radius <- 77 #the radius for clip_ircle()
for(i in 1:nrow(plots)){ #run the loop until i = the number of rows in 'plot_table' (20)
  plot_cent <- c(plots$X[i], plots$Y[i]) #extract plot center
  plot_las <- clip_circle(norm_mkrf, plot_cent[1], plot_cent[2], radius) #clip plot from norm_cat_las
  output_file <- paste("Plots_ITD/Plot_", i, ".las", sep = "") #output directory as string
  writeLAS(assign(paste("Plot_", i, sep = ""), plot_las), output_file) #write'MKRF_Plot_i' to output dir.
}
#calculate `cloud_metrics` for each plot
ITD_plot_metrics <- data.frame()
for(i in 1:nrow(plots)){ #for loop == number of rowns in plot_table (20)
  plot <- readLAS(paste("Plots_ITD/Plot_", i, ".las", sep= ""), filter = "-keep_first -drop_z_below 2") #keep only first returns and (65m > points > 2m)
  metrics <- cloud_metrics(plot, .stdmetrics) #compute metrics only on first return. Why?
  ITD_plot_metrics <- rbind(ITD_plot_metrics, metrics)
}
#segment each plot using li2012
p1_li = segment_trees(Plot_1, li2012())
plot(p1_li, color = 'treeID')
#segment each plot
p2_li = segment_trees(Plot_2, li2012())
plot(p2_li, color = 'treeID')
```

##### Q9: Describe a use case for delineating individual trees within a raw point cloud. {.unnumbered}

##### Screenshot 4: Include a figure of the output from segmenting each plot (p1, p2) {.unnumbered}

#### Part B) Detect individual trees using a CHM {.unnumbered}

In the previous section of this lab we used raw point cloud information to detect individual trees. Now, we will use a CHM in an attempt to increase classification accuracy. There are a variety of CHM-based segmentation algorithms within lidR, but we will use the one presented by Dalponte and Coomes 2016.

We will use the Dalponte et al., 2016 algorithm. Explore the Help page for this algorithm to understand it's details. You will notice that the arguments used in li2012 are different to those used in dalponte2016:

chm refers to a RasterLayer canopy height model.
treetops refers to the spatial location of individual trees determined by find_trees or an external shapefile

th_tree is the threshold below which a pixel cannot be a tree (default = 2)

th_seed is Growing Threshold 1 (default is 0.45)

th_cr is the Growing Threshold 2 (default is 0.55)

max_cr refers to the maximum value of the crown diameter of a detected tree (default = 10)

ID individual identification number of each tree.

Run **Part B**  of the script now. 

```{r, echo=TRUE, eval=FALSE}
### Part b) CHM based

#Dalponte et al., 2016 CHM-based 
#create CHM with 0.5m res
algo = pitfree(thresholds = c(0,10,20,30), max_edge = c(0,1), subcircle = 0.2)
chm_1_0.5  = grid_canopy(Plot_1, 0.5, algo)
plot(chm_1_0.5, col = height.colors(50))

#find treetops
ttops_1 <- find_trees(Plot_1, algorithm = lmf(5, shape = "circular"))
#add ttops_1 to chm
plot(ttops_1, add = TRUE) #adds ttops_1 to chm_1_0.5 plot; notice the variety of non-tree tops

dalponte_1_0.5 <- Plot_1 %>% 
  segment_trees(dalponte2016(chm_1_0.5, ttops_1)) %>% 
  filter_poi(!is.na(treeID))

crowns1 <- delineate_crowns(dalponte_1_0.5)
plot(crowns1, add = TRUE)

####  PLOT 2

#use dalponte with CHM for plot 2 at 0.5m resolution 
chm_2_0.5 <- grid_canopy(Plot_2, 0.5, algo)
plot(chm_2_0.5, col = height.colors(50))

ttops_2 <- find_trees(Plot_2, algorithm = lmf(5, shape = "circular"))
#add ttops_2 to chm2
plot(ttops_2, add = TRUE) #adds ttops_2 to chm_2_0.5 plot; notice the variety of non-tree tops

#apply to point cloud
dalponte_2_0.5 <- Plot_2 %>% 
  segment_trees(dalponte2016(chm_2_0.5, ttops_2)) %>% 
  filter_poi(!is.na(treeID))


#find crowns
crowns2 <- delineate_crowns(dalponte_2_0.5)
plot(crowns2, add = TRUE)
```

##### Screenshot 5: Include a figure of plot 2 showing the CHM within delineated crowns and tree tops {.unnumbered}

##### Q10: Compare the two CHM plots. In your opinion when does individual tree detection work well and where does it fall short. Explain. {.unnumbered}

#### Part C) Find tall trees using Point Cloud {.unnumbered}

`tree_metrics()` gives the ID of trees and associated metrics. We can use these data to filter the scene and isolate only the tallest trees. 

In this section we will isolate the largest of trees in plot 2. 

Run **Part C**  of the script now.

```{r, echo=TRUE, eval=FALSE}

### Part C finding Tall Trees in Point Clouds
metrics <- tree_metrics(p2_li, ~list(Zmax = max(Z))) # calculate tree height metrics
metrics <- metrics[metrics$Zmax > 45, ] #filter for trees taller than 45 m 

subset <- filter_poi(p2_li, treeID %in% metrics$treeID) #subset point cloud

x <- plot(p2_li, bg = "white", size = 4)
plot(subset, add = x + c(-200, 0), size = 5) # some plotting

#Extract and plot tallest tree
maxZ = max(metrics$Zmax)
metricsmax <- metrics[metrics$Zmax == maxZ, ] #filter for maxZ 

Subsetmax <- filter_poi(p2_li, treeID %in% metricsmax$treeID) #subset point cloud

x <- plot(p2_li, bg = "white", size = 4)
plot(Subsetmax, add = x + c(-200, 0), size = 5) # some plotting

plot(Subsetmax, bg = "white")
```

##### Screenshot 6: Include an image of the subsetmax plot {.unnumbered}

------------------------------------------------------------------------
## Summary {.unnumbered}

We started this lab with learning to install R and Rstudio and progressed quickly towards visualizing RGB and point cloud data, creating a DEM and CHM, extracting cloud_metrics, using an OLS regression to preduct forest attributes with lidar metrics, and using individual tree detection methods to find tall trees using points clouds. Packages like lidR and tidyverse allow us to conduct meaningful and important analysis with relatively few lines of code. This lab was just a few examples of important questions about our forests that can be modeled and answered using these tools! When moving forward and including these methods into your own workflows, remember the importance of getting to know your data and modeling methods. Ask yourself, what are these duplicate points doing to my processing times? How are these outlying points affecting my data? What model works best for the question I am trying to answer with my metrics? Are these variables equally important or are they highly correlated and I am fitting noise?

Return to the [**Deliverables**](#lab3-deliverables) section to check off everything you need to submit for credit in the course management system.
